

1. This is the policy learned by Q-table version Q-learning.

M R D R R L L R R L R D L L L L L L D R R R R R R R R R R R D R R D L L M
M R L U U D L L R D L L D D D L L D D D D D D D R R R D D L L D R L D L M
M R R D R R L R U L L D L L L L D D D D D D R D R R D D U L R U U L R L M
M R R D U L D U D D D D R D L D D D D D D D R R D R D D L U R L U U U L M
M R R U L D D D R U D D L D L L D D D D D D D L D D D D R U D D R U R L M
M R R U D D D D D L D L L D L L D D D D D D L D D R R D U L R D R D D L M
M R R U D U U R U D L U L U R L D D D D D D R D R D R D R D R L R L L L M
M R L R D D D L D D L L D D D D D D D D D D R D D D D R R D R U D D R L M
M R R D U R R D L D R D D L D D D D D D D D R D D R L D D R D D R U L L M
M R L L D D D D D D D D D D D D D D D D D D D R D D D D D D D D D R L L M
M R U D L D D D D D D D L D D D D D D D D D D U R D L D D D R L L U U L M
M R D R D L D D L L L L D R R D D D D D D D D D R U R D D D D D D U L L M
M R L D D D D D D D D D D D D D D D D D D D D D L D R D R D L L D R D L M
M R R R D L D R D U D L D D D D D D D D D D D U D D D R L D R D L D D L M
M R D D L D D L D D D L D L D D D D D D D D D D D D L D D D D D U U U L M
M R D D D D D R L D D U D D D D D D D D D D D D L D R D U D L D D D D L M
M R D D R D L D D L D D D D D D D D D D D D D D D D D D D U D D D D L L M
M R D R D U R D D D L D D R D D D D D D D D D D L L D D D D D D U D D L M
M R D U D D D D U R D D D D D D D D D D D D D D D D U D D D U D D U D L M
M R D D L L D D D D D R D R D D D D D D D D D D D D D D D D D D R D L L M
M R D D D R D D D R U D D D D D D D D D D D D D D D D R L L D R U D U L M
M R U D U D D L D D D D D D D D D D D D D D D D D D D L L L U D D D D L M
M R D U L D L R D D U D D D D D D D D D D D D D D D D D D D D L L L D L M
M R R D D D D L D D D R D D D D D D D D D D D D D D D D D D D R L L L L M
M R D D D R D D L D D D D D D D D D D D D L D D D D D D R D L D R L U L M
M R D L D D D L R D D D D D D D D D D D D D D D D D L D D D D L D D U L M
M R L R D D D D D R D D D D D D D D D D D D D D D D D U L D R D U L D L M
M R R D D D D D R D D D R D D D D D D D D D D D D D R D D L D U D D D L M
M R D D D D D D D D L D D D D D D D D D D D D D D D D D D D D D U D L L M
M R R L R R U D D D D R D D D D D D D D D D D L L D D D L D D U D D D L M
M R U D R D D D R D R D D D D R D D D D D D D D D D L D U U D D D D D L M
M R D D D D D D L D D D D D D D D D D D D D D L L L D L D D D L D U L L M
M R U L R D D L D D D R D D D D D D D D D D D D D D D D D D R D D L D L M
M R R D D D D D D D D D R D R D D D D D D D L L D L D D L D D D D D D L M
M R D D D D U D D D D D D D D D D D D D D D D L L D D L D R L D D D U L M
M R D D D D L R D D D D D D D D D D D D D D L D L L D D D D D L L D D L M
M R L D D D D D R D D D R D R D D D D D D D D D L D D L D D L D D D D L M
M R D D D R D L R R R R R R D D D D D D D D D D D D D D L D L D D D U L M
M R D L L R R D D D D R D R D D D D D D D D D L L D D L D R D D L D U L M
M R D R D D D D R D R R R D R D D D D D D D D D D D D L L D D D D D D L M
M R D R D D U D R D D D R D R D D D D D D L L D L L L L L L L D D U D L M
M R D D D L D L D D D D R D D R D R D D D D D L L D L D L D L L D U D L M
M R D R D R D R R D D D D D D D D D D D D D L L D L D L D D D D R R D D M
M R D D U R D D D D R D D R R D D D D D D D L D L L L L L L D D D D D D M
M D D D D D D D R D D R R R D R D D D D D L L L L L D L L D D D D D D L M
M R D D U R D R R D R R R R R R R D D D D L L D D D D L L L L U L R D L M
M R D D R R D D R R D D D D R R R D D D D L L D D L D D L D L R L D L L M
M R R R R R R R R D D R D R D R R D R D L L L L L D D D D L D L L U R L M
M R L R R R R R D R R R R R R D R D D D D L L L L L L L L L L L L L R L M
M R R R R R R R R R R R R R R R R R G L L L L L L L L L L L L L U L U L M


2. You can check graph called Q-tableReward in archive, and i post one more graph called Q-tableRewardsDetail which show the details of line so that you can see the line fluctuates.


3.First of all, I think the final policy is right. You can clearly find the way from start to gate that all the way down.And you can find the grid nearby mines are always in the opposite direction.And you may also find some particular points' direction is wrong, because not every state will update value when we find the gate.Some of them will be stay in wrong value.But our policy coverages to the final and nearest way to gate.
  Secondly,You can view the graph called Q-tableReward in archive, which is average reward of testing, you can clearly find at the beginning of learning. Our program is always go to randomly the state that we do not learn before, so it cause we will go through lot of state we don't know and that make we waste lot of step. After we do more learning, we will explore more state we don't know, and it will make program easily find the gate. After 25 rounds tests, the average reward coverage around 55.Because agent have the probability to  slips to one side or the other ,it still fluctuate and the value of reward is depend on how many times agent slips .
  
 
