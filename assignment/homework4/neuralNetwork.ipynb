{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "from keras.datasets import cifar10\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.constraints import maxnorm\n",
    "from keras.optimizers import SGD\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.utils import np_utils\n",
    "from keras import backend as K\n",
    "import tensorflow\n",
    "K.set_image_dim_ordering('th')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restricting GPU memory usage\n",
    "\n",
    "The code here should be added to any work you do on Volta.  If you don't, then your code will monopolize all available memory on each of the 4 GPUs on the machine, preventing others from working on it.  If you do **that**, you will be frowned upon.\n",
    "\n",
    "The code in the next cell has the effect that:\n",
    "1. Memory use will start off with some small fraction of the memory on each GPU.\n",
    "1. It will grow if necessary (since `allow_growth` is set to `True`).\n",
    "1. It will max out at 5% of overall memory.  Given the GPUs we have, this gives you (4 x 808 MB), which should be sufficient here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tensorflow.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.05\n",
    "K.tensorflow_backend.set_session(tensorflow.Session(config=config))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load cifar10 image_data\n",
    "1. First of all, load traindata and traindata from cifar10. `x_train,x_test` are uint8 array of RGB image data ,and `y_train,Y_table` are  uint8 array of category labels (integers in range 0-9).\n",
    "1. Initialize the random number seed with a constant to ensure the results are reproducible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "seed = 7\n",
    "numpy.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization of data\n",
    "Every time we should normalization data( let data from(0,1)).\n",
    "The pixel values are in the range of 0 to 255 for each of the red, green and blue channels.\n",
    "It is good practice to work with normalized data. Because the input values are well understood, we can easily normalize to the range 0 to 1 by dividing each value by the maximum observation which is 255.\n",
    "Note, the data is loaded as integers, so we must cast it to floating point values in order to perform the division."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform labal_data to binary matrix\n",
    "1. Using`np_utils.to_categorical()` method to let labal be onehot format like(1,0,0,0,0,0,0,0,0,0).This binary format let algrorithm's prediction be easy. \n",
    "2. Give the number of labal to num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_test = np_utils.to_categorical(y_test)\n",
    "num_classes = y_test.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Built the model's layers\n",
    "1. Sequential model is a linear stack of layers. You can add layers to build model you want.\n",
    "\n",
    "2.  This is a shape tuple.Conv2D layer creates a convolution kernel that is convolved with the layer input to produce a tensor of outputs.the input parameter`32`means output space have 32  dimensionality, `(3,3)` means height and width of the 2D convolution window, Fist layer you must Pass `input_shape` argument`(3,32,32)`means 32x32 RGB pictures in  data_format=\"channels_last\",activation function is `relu`   ![image](  data:image/png;base64,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)    `padding=same`means you can have 0 to handle with padding.\n",
    "\n",
    "1. Dropout is a technique where randomly selected neurons are ignored during training. They are “dropped-out” randomly. In this model we set drop probability to 0.2. It is good technology to make model genenralize well.\n",
    "\n",
    "1. MaxPooling2D(pool_size=(2,2)) is also the layer to prevent overfitting and also can decrease calculation step. `pool_size=(2,2)` means downscale (vertical, horizontal). (2, 2) will halve the input in both spatial dimension.\n",
    "\n",
    "1. `Flatten()` means let the input multidimension data change to one dimension.\n",
    "\n",
    "1. Dense(1024, activation='relu', kernel_constraint=maxnorm(3)) implements the operation: `output = activation(dot(input, kernel) + bias)`. The parameter`1024` means output space have 1024 dimentionaliity.'activation='relu' means use relu function to be activation function. `kernel_constraint=maxnorm(3)` means constraint function applied to the kernel weights matrix .\n",
    "\n",
    "1. model.add(Dense(num_classes, activation='softmax')) means the last layer will give 10 dimentionality output and use `softmax `to be activation function   ![image]( https://upload-images.jianshu.io/upload_images/1531909-a0d0107595ea6f36.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/632/format/webp)       \n",
    "### Test\n",
    "I try to add 4 layers to this model but it is useless ,and it will decrease the accurcy. I think the reason is too much convolution layers let the model loss a lot feature, and still use `dropout`and `MaxPooling2D` to prevent overfitting ,it makes performance decrease.So i just add one convolution layer, but the accuray can be 89.5% ,it is not increase a lot.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()#1\n",
    "model.add(Conv2D(32, (3, 3), input_shape=(3, 32, 32), activation='relu', padding='same'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', padding='same'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(48, (3, 3), activation='relu', padding='same'))\n",
    "# model.add(Dropout(0.2))\n",
    "# model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1024, activation='relu', kernel_constraint=maxnorm(3)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(512, activation='relu', kernel_constraint=maxnorm(3)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## compile model\n",
    "1. We set epochs to 200 means data will loop 200 times to training network.\n",
    "3. We use Stochastic Gradient Descent (SGD) to be optimizer to get what value can let loss function be small.`SGD(lr=lrate, momentum=0.9, decay=decay, nesterov=False)` .we set learning rate to 0.02,and `decay` means learning rate will decy in every loop for epochs.`neterov=False` means do not use Nesterov momentum.\n",
    "4.  `compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])`. we use `categorical_crossentropy`to be loss function,and  `metrics`: List of metrics to be evaluated by the model during training and testing. Typically you will use  metrics=['accuracy'].\n",
    "### Test\n",
    "I try to use `adddelta` to be the optimizer, but result is not good. Accuracy is only 78.9%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 32, 32, 32)        896       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 32, 32, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 32, 16, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 48, 16, 16)        13872     \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 64, 16, 16)        27712     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 64, 16, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 64, 16, 16)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 64, 8, 8)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 128, 8, 8)         73856     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128, 8, 8)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 128, 8, 8)         147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 128, 4, 4)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1024)              2098176   \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 2,938,202\n",
      "Trainable params: 2,938,202\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Compile model\n",
    "epochs = 100\n",
    "lrate = 0.02\n",
    "decay = lrate/epochs\n",
    "sgd = SGD(lr=lrate, momentum=0.9, decay=decay, nesterov=False)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "# model.compile(loss='categorical_crossentropy', optimizer='adadelta', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data augumentation\n",
    "1. I use keras.preprocessing.image.ImageDataGenerator() to relize image data augumentation,\n",
    "  `featurewise_center=False` set input mean to 0 over the dataset,\n",
    "  `samplewise_center=False`  set each sample mean to 0,\n",
    "  ` featurewise_std_normalization=False`  divide inputs by std of the dataset,\n",
    "  `samplewise_std_normalization=False` divide each input by its std,\n",
    "  ` zca_whitening=False`   apply ZCA whitening,\n",
    "  `rotation_range=10`  randomly rotate images in the range (degrees, 0 to 180),\n",
    "  `width_shift_range=0.2` randomly shift images horizontally (fraction of total width),\n",
    "  `height_shift_range=0.2`  randomly shift images vertically (fraction of total height),\n",
    "  `horizontal_flip=True`   randomly flip images.\n",
    "1. `augumentation.fit(X_train)` is method to use `ImageDataGenerator` to deal with training data.\n",
    "2. `augumentation.flow(X_train, y_train)` return an iterator yielding tuples of (x, y).\n",
    "3. `model.evaluate(x_test,y_test,verbose=0)`is the method to evaluate model.`verbose=0`means silent.\n",
    "### Test\n",
    "After i use augumentation, the accuracy increase to 89%.It is really useful! And i find that if your `batch_size` set large ,the accuracy will be lower.I think if the `batch_size`is too large.Because of `SGD` feature, we will be easy to lock in local minimum point.But if `batch_size` set small,the network will be more randomly so that it is more possible to break local mimimun point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5000\n",
      "5000/5000 [==============================] - 144s 29ms/step - loss: 1.5730 - acc: 0.4222 - val_loss: 1.1472 - val_acc: 0.5884\n",
      "Epoch 2/5000\n",
      "5000/5000 [==============================] - 138s 28ms/step - loss: 1.1298 - acc: 0.5987 - val_loss: 0.8993 - val_acc: 0.6867\n",
      "Epoch 3/5000\n",
      "5000/5000 [==============================] - 138s 28ms/step - loss: 0.9930 - acc: 0.6519 - val_loss: 0.7918 - val_acc: 0.7255\n",
      "Epoch 4/5000\n",
      "5000/5000 [==============================] - 141s 28ms/step - loss: 0.9309 - acc: 0.6768 - val_loss: 0.7522 - val_acc: 0.7436\n",
      "Epoch 5/5000\n",
      "4350/5000 [=========================>....] - ETA: 18s - loss: 0.9010 - acc: 0.6895"
     ]
    }
   ],
   "source": [
    "augumentation=ImageDataGenerator(\n",
    "        featurewise_center=False,  \n",
    "        samplewise_center=False,  \n",
    "        featurewise_std_normalization=False,  \n",
    "        samplewise_std_normalization=False,  \n",
    "        zca_whitening=False, \n",
    "        rotation_range=10,  \n",
    "        width_shift_range=0.2,  \n",
    "        height_shift_range=0.2,  \n",
    "        horizontal_flip=True,  \n",
    "        vertical_flip=False) \n",
    "augumentation.fit(X_train)\n",
    "gen = augumentation.flow(X_train, y_train,batch_size=64)\n",
    "model.fit_generator(generator=gen,steps_per_epoch=5000, \n",
    "                                 epochs=epochs, \n",
    "                                 validation_data=(X_test, y_test))\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
